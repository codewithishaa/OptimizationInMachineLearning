{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements to a Neural Network\n",
    "\n",
    "In a previous notebook we implemented a simple feedforward neural network and used it to classify handwritten digits. In this notebook we modify the simple network, adding several improvements which improve the accuracy of the network. As before, the code in this notebook is an adaption of the code (network2.py) included with the book \"[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\" by Michael Nielsen, specifically the [version and adapted to work with Python 3](https://github.com/MichalDanielDobrzanski/DeepLearningPython) by Michal Daniel Dobrzanski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful libraries and convenience functions\n",
    "\n",
    "First, we load some standard Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a convenience function for identifying which digit the network recognised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for loading a previously-saved network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our activation function and another function to compute its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost functions\n",
    "\n",
    "We will be adding support in our Network class for different choices of cost function. For this, a cost function should be defined as a class  with two functions:\n",
    "* fn(a, y): compute the cost for a given a and y\n",
    "* delta: compute the error, $\\delta^L$ in the output layer.\n",
    "\n",
    "We now define two specific cost functions\n",
    "\n",
    "#### Quadratic cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Cross-entropy cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return (a-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class\n",
    "\n",
    "Next, we define a class \"Network\" that will represent our neural network. This includes several modifications to the previous class, primarily to allow for different cost functions, regularization, better weight initialization, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False,\n",
    "            early_stopping_n = 0):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # early stopping functionality:\n",
    "        best_accuracy=1\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if evaluation_data:\n",
    "            evaluation_data = list(evaluation_data)\n",
    "            n_data = len(evaluation_data)\n",
    "\n",
    "        # early stopping functionality:\n",
    "        best_accuracy=0\n",
    "        no_accuracy_change=0\n",
    "\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "\n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
    "\n",
    "            # Early stopping:\n",
    "            if early_stopping_n > 0:\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    no_accuracy_change = 0\n",
    "                    #print(\"Early-stopping: Best so far {}\".format(best_accuracy))\n",
    "                else:\n",
    "                    no_accuracy_change += 1\n",
    "\n",
    "                if (no_accuracy_change == early_stopping_n):\n",
    "                    #print(\"Early-stopping: No accuracy change in last epochs: {}\".format(early_stopping_n))\n",
    "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "\n",
    "        result_accuracy = sum(int(x == y) for (x, y) in results)\n",
    "        return result_accuracy\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights) # '**' - to the power of.\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We now look at a simple example, running the new network with the quadratic cost function, no regularization and the original method for initializing the weights. First, we load the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 8264 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 8476 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9300 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9306 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9378 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9368 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9382 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9397 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9414 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [8264, 8370, 8476, 9300, 9306, 9378, 9368, 9382, 9397, 9414], [], [])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([784, 30, 10], cost=QuadraticCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 10, 10, 3.0, evaluation_data=test_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Cross-entropy cost function\n",
    "1. Train network with a single hidden layer with 30 neurons using the cross-entropy cost function. Note: you should use a different learning rate to produce comparable results. A good rule of thumb is to reduce the rate by a factor of 6 when using a cross-entropy cost function compared to a quadratic cost function.\n",
    "2. Compare the accuracy against the quadratic cost case.\n",
    "3. Repeat, but with 100 neurons.\n",
    "\n",
    "### Overfitting and regularization\n",
    "1. Train a 30 neuron network with a 1000-sample subset of the MNIST training data. Use the cross-entropy cost function and train for 400 epochs. Enabe monitoring of the evaluation accuracy and training cost and plot the results.\n",
    "2. Train the network again, but this time including regularization by setting `lmbda=0.1`.\n",
    "3. Compare the results in the two cases.\n",
    "\n",
    "\n",
    "### Weight initialization\n",
    "1. Train your network again, but this time use the improved `default_weight_initializer` weight initialization.\n",
    "2. Compare the accuracy with previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9129 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9271 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9374 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9397 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9438 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9440 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9424 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9455 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9482 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9462 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [9129, 9271, 9374, 9397, 9438, 9440, 9424, 9455, 9482, 9462], [], [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 10, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 8340 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 8495 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9327 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9484 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9477 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9499 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9551 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9565 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9562 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9575 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [8340, 8495, 9327, 9484, 9477, 9499, 9551, 9565, 9562, 9575], [], [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "net = Network([784, 100, 10], cost=QuadraticCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 10, 10, 3.0, evaluation_data=test_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9327 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9485 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9533 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9600 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9620 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9632 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9608 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9651 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9666 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9652 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [9327, 9485, 9533, 9600, 9620, 9632, 9608, 9651, 9666, 9652], [], [])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "net = Network([784, 100, 10], cost=CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 10, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 1.4860090005504503\n",
      "Accuracy on evaluation data: 617 / 1000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.8280943195441068\n",
      "Accuracy on evaluation data: 694 / 1000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.5366466416989764\n",
      "Accuracy on evaluation data: 767 / 1000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.403756159940483\n",
      "Accuracy on evaluation data: 755 / 1000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.3085568322759454\n",
      "Accuracy on evaluation data: 757 / 1000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.24047572413134766\n",
      "Accuracy on evaluation data: 778 / 1000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.1734732106297779\n",
      "Accuracy on evaluation data: 789 / 1000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.15306847764062245\n",
      "Accuracy on evaluation data: 800 / 1000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.13873407517143088\n",
      "Accuracy on evaluation data: 781 / 1000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.09949106892934292\n",
      "Accuracy on evaluation data: 796 / 1000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.0861098563746581\n",
      "Accuracy on evaluation data: 799 / 1000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.08588264128582065\n",
      "Accuracy on evaluation data: 795 / 1000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.08057020314104271\n",
      "Accuracy on evaluation data: 799 / 1000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.06412690357074638\n",
      "Accuracy on evaluation data: 798 / 1000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.057007320417624796\n",
      "Accuracy on evaluation data: 798 / 1000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.05636996937438226\n",
      "Accuracy on evaluation data: 804 / 1000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.04899514365465413\n",
      "Accuracy on evaluation data: 801 / 1000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.045780613383313656\n",
      "Accuracy on evaluation data: 806 / 1000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.044457387388562794\n",
      "Accuracy on evaluation data: 802 / 1000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.041909051822022945\n",
      "Accuracy on evaluation data: 808 / 1000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.038023942839024855\n",
      "Accuracy on evaluation data: 808 / 1000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.03542708141594091\n",
      "Accuracy on evaluation data: 807 / 1000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.03438673641117075\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.03207797345911388\n",
      "Accuracy on evaluation data: 804 / 1000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.0306379915765604\n",
      "Accuracy on evaluation data: 805 / 1000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.029842836471050224\n",
      "Accuracy on evaluation data: 805 / 1000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.027747991401994767\n",
      "Accuracy on evaluation data: 810 / 1000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.026805623381960374\n",
      "Accuracy on evaluation data: 805 / 1000\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.02583137206903195\n",
      "Accuracy on evaluation data: 808 / 1000\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.024733692891839\n",
      "Accuracy on evaluation data: 809 / 1000\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.023809926066772978\n",
      "Accuracy on evaluation data: 808 / 1000\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.02290680118412933\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.02216706481925767\n",
      "Accuracy on evaluation data: 810 / 1000\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.021330946494124023\n",
      "Accuracy on evaluation data: 810 / 1000\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.02058596223527525\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.019936403066641177\n",
      "Accuracy on evaluation data: 810 / 1000\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.01938996346324361\n",
      "Accuracy on evaluation data: 809 / 1000\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.01872798221027592\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.018242870854387385\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.01772315087742149\n",
      "Accuracy on evaluation data: 810 / 1000\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.017291433653981874\n",
      "Accuracy on evaluation data: 809 / 1000\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.016770699400081312\n",
      "Accuracy on evaluation data: 810 / 1000\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.016319402998070203\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.015916193218857986\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.015488844470941449\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.0151227226752806\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.01478497914908075\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.014424744271281579\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.014099577552351625\n",
      "Accuracy on evaluation data: 809 / 1000\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.013778930671771905\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.013474430337901062\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.013170758762086366\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.012943750226847845\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.012608666228305894\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.012416375624288377\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.01214516153239222\n",
      "Accuracy on evaluation data: 811 / 1000\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.011915399712760955\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.011631786233707789\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.011410939167039504\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.01121277573067292\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.010985912492821754\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.01079724510469282\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.01060963850261874\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.01043339017069026\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.010280439363227914\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.010074934469319854\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.00991509037850457\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.009745938082216923\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.00958383195798235\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.009436334303025392\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.0092998081841727\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.00914619049997124\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.009010107832255832\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.008868725258129215\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.008738667706135262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.00860857454495136\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.00848546245568806\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.008372491425575869\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.008256239836670941\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.008135202094298369\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.008026750553486157\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.007912340064243024\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.007812776498783147\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.007706704179971372\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.007602183694404931\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.007504581310953663\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.00741141715784975\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.007316186143001194\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.00722918598136662\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.007144319199325253\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.00705308311134918\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.006975796830794571\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.006883671864217525\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.006799839305481967\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.0067207811392139055\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.006642694582703226\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.0065715902718388695\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.00649456614779212\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.0064264775364375\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.006349202944328813\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.006280766364063707\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.006212864529105994\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.006147076541728791\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.006079020910126854\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.0060171595592770615\n",
      "Accuracy on evaluation data: 812 / 1000\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.0059539734305073195\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.005893434372808843\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.005837723243024336\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.005771762728438364\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.005716635737530476\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.005657680855850348\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.005602420435005678\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.00554704922817219\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.005493515299150136\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.005441014936240289\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.005390834313913964\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.005339636684147621\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.005288886905794968\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.0052381816913798455\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.0051927983948177\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.005142661671939819\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.005096937383336491\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.005051069997846578\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.005005730272522156\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.004962561297957301\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.004918123334006114\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.004875024066388754\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.004834427579104659\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.004793123268166903\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.004751393844793896\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.004711870233694022\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.0046711536004299635\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.004633151677519255\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.004594160567127169\n",
      "Accuracy on evaluation data: 813 / 1000\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.004556854846722735\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.004520236671184956\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.004483010948461276\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.00444764662052496\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.004412510849434112\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.004376878352703191\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.004342726109054858\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.004309328539642442\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.004275652995946331\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.004243108988262437\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.004212251081219184\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.004179122412973893\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.004147499637035717\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.0041166954921723784\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.004086216916705137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.004055702011084691\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.004026487261793825\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.003996695039947755\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.003968026085300022\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.003940351293773149\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.003911257112385546\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.003884147860876704\n",
      "Accuracy on evaluation data: 814 / 1000\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.003856710417840155\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.0038299541324404093\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.003802872767643012\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.0037769695654307333\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.003750648205435093\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.003725303595316258\n",
      "Accuracy on evaluation data: 815 / 1000\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.0037003529847531623\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.00367507909554692\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.003650555132699767\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.003626050575324353\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.003602208273178827\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.003578367390085992\n",
      "Accuracy on evaluation data: 816 / 1000\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.0035548296005908594\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.003531936022282689\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.0035092471792538254\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.0034864162565246427\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.003464149331840719\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.003442184206225732\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.003421009256987645\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.003399776074507443\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.0033781302264436278\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.0033571757509373314\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.00333609039161036\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.0033156348994267883\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.003295412958850589\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.0032753055743751655\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.0032554910776436572\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.003236012548416205\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.0032167963061466276\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.0031983227856402416\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.003178670809822685\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.003160103155455913\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.003141722198896312\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.0031235852540828497\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.003105324993008273\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.003087356385405269\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.003069740444953951\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.0030521662988711065\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.003035103339522621\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.0030178643170641448\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.0030008868226569993\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.0029842368975238204\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.0029677781369283063\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.002951143357958223\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 200 training complete\n",
      "Cost on training data: 0.002935185612755114\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 201 training complete\n",
      "Cost on training data: 0.0029189238772388206\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 202 training complete\n",
      "Cost on training data: 0.00290315926216028\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 203 training complete\n",
      "Cost on training data: 0.0028874331976949715\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 204 training complete\n",
      "Cost on training data: 0.002871964128799633\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 205 training complete\n",
      "Cost on training data: 0.00285640560860618\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 206 training complete\n",
      "Cost on training data: 0.002841337781323861\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 207 training complete\n",
      "Cost on training data: 0.0028261066434823448\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 208 training complete\n",
      "Cost on training data: 0.0028113507073558234\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 209 training complete\n",
      "Cost on training data: 0.0027963923862072532\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 210 training complete\n",
      "Cost on training data: 0.0027819825223036956\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 211 training complete\n",
      "Cost on training data: 0.0027672196474394332\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 212 training complete\n",
      "Cost on training data: 0.0027528228214703035\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 213 training complete\n",
      "Cost on training data: 0.0027387296231636063\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 214 training complete\n",
      "Cost on training data: 0.002724710148602946\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 215 training complete\n",
      "Cost on training data: 0.0027107752053224764\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 216 training complete\n",
      "Cost on training data: 0.0026970430818257945\n",
      "Accuracy on evaluation data: 817 / 1000\n",
      "Epoch 217 training complete\n",
      "Cost on training data: 0.0026832529538517867\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 218 training complete\n",
      "Cost on training data: 0.0026697258273466785\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 219 training complete\n",
      "Cost on training data: 0.0026563599604643258\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 220 training complete\n",
      "Cost on training data: 0.0026431054998058345\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 221 training complete\n",
      "Cost on training data: 0.002630115969441326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 222 training complete\n",
      "Cost on training data: 0.0026169942435038795\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 223 training complete\n",
      "Cost on training data: 0.002604044933142382\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 224 training complete\n",
      "Cost on training data: 0.0025913000085283568\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 225 training complete\n",
      "Cost on training data: 0.0025786345594925658\n",
      "Accuracy on evaluation data: 818 / 1000\n",
      "Epoch 226 training complete\n",
      "Cost on training data: 0.0025661289548268985\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 227 training complete\n",
      "Cost on training data: 0.0025536455671429903\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 228 training complete\n",
      "Cost on training data: 0.002541405357160823\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 229 training complete\n",
      "Cost on training data: 0.0025292313886642833\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 230 training complete\n",
      "Cost on training data: 0.0025172730038195415\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 231 training complete\n",
      "Cost on training data: 0.0025052131902663817\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 232 training complete\n",
      "Cost on training data: 0.002493346792981853\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 233 training complete\n",
      "Cost on training data: 0.002481706797298765\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 234 training complete\n",
      "Cost on training data: 0.002470100398863122\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 235 training complete\n",
      "Cost on training data: 0.0024585142801331664\n",
      "Accuracy on evaluation data: 819 / 1000\n",
      "Epoch 236 training complete\n",
      "Cost on training data: 0.0024469939006431587\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 237 training complete\n",
      "Cost on training data: 0.0024356106513411554\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 238 training complete\n",
      "Cost on training data: 0.0024244640318756244\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 239 training complete\n",
      "Cost on training data: 0.0024132624056740727\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 240 training complete\n",
      "Cost on training data: 0.00240228807152328\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 241 training complete\n",
      "Cost on training data: 0.0023912944298381616\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 242 training complete\n",
      "Cost on training data: 0.002380414931706105\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 243 training complete\n",
      "Cost on training data: 0.002369734397461747\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 244 training complete\n",
      "Cost on training data: 0.002359130612975542\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 245 training complete\n",
      "Cost on training data: 0.002348482192209523\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 246 training complete\n",
      "Cost on training data: 0.0023380306276204626\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 247 training complete\n",
      "Cost on training data: 0.0023277815677281532\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 248 training complete\n",
      "Cost on training data: 0.0023174375120689947\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 249 training complete\n",
      "Cost on training data: 0.0023071222152245955\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 250 training complete\n",
      "Cost on training data: 0.002297172777776851\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 251 training complete\n",
      "Cost on training data: 0.0022871295998763202\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 252 training complete\n",
      "Cost on training data: 0.002277158202351896\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 253 training complete\n",
      "Cost on training data: 0.0022672509298639772\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 254 training complete\n",
      "Cost on training data: 0.0022575724220680457\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 255 training complete\n",
      "Cost on training data: 0.0022476990381330507\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 256 training complete\n",
      "Cost on training data: 0.0022381840133002835\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 257 training complete\n",
      "Cost on training data: 0.002228529845097418\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 258 training complete\n",
      "Cost on training data: 0.002219041950034404\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 259 training complete\n",
      "Cost on training data: 0.0022097313646171715\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 260 training complete\n",
      "Cost on training data: 0.0022003742466529335\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 261 training complete\n",
      "Cost on training data: 0.0021911896760226364\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 262 training complete\n",
      "Cost on training data: 0.0021819926376587103\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 263 training complete\n",
      "Cost on training data: 0.0021729132864321187\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 264 training complete\n",
      "Cost on training data: 0.0021639367813171832\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 265 training complete\n",
      "Cost on training data: 0.0021549587411979257\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 266 training complete\n",
      "Cost on training data: 0.002146078394093681\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 267 training complete\n",
      "Cost on training data: 0.0021372535347313126\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 268 training complete\n",
      "Cost on training data: 0.0021285537880688715\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 269 training complete\n",
      "Cost on training data: 0.0021199290631801334\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 270 training complete\n",
      "Cost on training data: 0.0021112606884334546\n",
      "Accuracy on evaluation data: 820 / 1000\n",
      "Epoch 271 training complete\n",
      "Cost on training data: 0.002102759789059166\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 272 training complete\n",
      "Cost on training data: 0.0020943134090421495\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 273 training complete\n",
      "Cost on training data: 0.002085916914943167\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 274 training complete\n",
      "Cost on training data: 0.0020775766311441806\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 275 training complete\n",
      "Cost on training data: 0.0020692937125251086\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 276 training complete\n",
      "Cost on training data: 0.002061140454812675\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 277 training complete\n",
      "Cost on training data: 0.002052970875001824\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 278 training complete\n",
      "Cost on training data: 0.002044855853372872\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 279 training complete\n",
      "Cost on training data: 0.002036862517121067\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 280 training complete\n",
      "Cost on training data: 0.0020288958382185624\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 281 training complete\n",
      "Cost on training data: 0.002021005176527842\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 282 training complete\n",
      "Cost on training data: 0.0020131527934101395\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 283 training complete\n",
      "Cost on training data: 0.002005377283007738\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 284 training complete\n",
      "Cost on training data: 0.0019976816396060464\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 285 training complete\n",
      "Cost on training data: 0.0019900419144368263\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 286 training complete\n",
      "Cost on training data: 0.001982394557380568\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 287 training complete\n",
      "Cost on training data: 0.001974869269795398\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 288 training complete\n",
      "Cost on training data: 0.001967386646226768\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 289 training complete\n",
      "Cost on training data: 0.001959967544210675\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 290 training complete\n",
      "Cost on training data: 0.0019525751332373608\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 291 training complete\n",
      "Cost on training data: 0.0019452528944115745\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 292 training complete\n",
      "Cost on training data: 0.0019379433276990542\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 293 training complete\n",
      "Cost on training data: 0.001930708549982476\n",
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 294 training complete\n",
      "Cost on training data: 0.0019235697192032137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 821 / 1000\n",
      "Epoch 295 training complete\n",
      "Cost on training data: 0.0019163996527454892\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 296 training complete\n",
      "Cost on training data: 0.00190932912335846\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 297 training complete\n",
      "Cost on training data: 0.0019023088684398197\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 298 training complete\n",
      "Cost on training data: 0.001895342286152582\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 299 training complete\n",
      "Cost on training data: 0.0018884094379651888\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 300 training complete\n",
      "Cost on training data: 0.001881537622707725\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 301 training complete\n",
      "Cost on training data: 0.0018747695136692548\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 302 training complete\n",
      "Cost on training data: 0.0018679846481921593\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 303 training complete\n",
      "Cost on training data: 0.001861208845394822\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 304 training complete\n",
      "Cost on training data: 0.0018545179941365304\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 305 training complete\n",
      "Cost on training data: 0.0018478940953408414\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 306 training complete\n",
      "Cost on training data: 0.0018413328970543373\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 307 training complete\n",
      "Cost on training data: 0.0018347782788468501\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 308 training complete\n",
      "Cost on training data: 0.0018282541840208915\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 309 training complete\n",
      "Cost on training data: 0.0018217974158265511\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 310 training complete\n",
      "Cost on training data: 0.001815365655454888\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 311 training complete\n",
      "Cost on training data: 0.0018089998071789627\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 312 training complete\n",
      "Cost on training data: 0.001802670886038719\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 313 training complete\n",
      "Cost on training data: 0.0017963834551463463\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 314 training complete\n",
      "Cost on training data: 0.0017901521905088673\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 315 training complete\n",
      "Cost on training data: 0.0017839543867796874\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 316 training complete\n",
      "Cost on training data: 0.001777811896929545\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 317 training complete\n",
      "Cost on training data: 0.0017716844477538188\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 318 training complete\n",
      "Cost on training data: 0.0017656004521386381\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 319 training complete\n",
      "Cost on training data: 0.0017595690507968987\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 320 training complete\n",
      "Cost on training data: 0.0017535856735146549\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 321 training complete\n",
      "Cost on training data: 0.0017476170994782392\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 322 training complete\n",
      "Cost on training data: 0.0017417535309568082\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 323 training complete\n",
      "Cost on training data: 0.001735827307003468\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 324 training complete\n",
      "Cost on training data: 0.0017299928602195808\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 325 training complete\n",
      "Cost on training data: 0.00172420173079514\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 326 training complete\n",
      "Cost on training data: 0.0017184240401416603\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 327 training complete\n",
      "Cost on training data: 0.0017127075306587899\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 328 training complete\n",
      "Cost on training data: 0.0017069948464568032\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 329 training complete\n",
      "Cost on training data: 0.0017013444954066814\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 330 training complete\n",
      "Cost on training data: 0.0016957691392599059\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 331 training complete\n",
      "Cost on training data: 0.001690154214809408\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 332 training complete\n",
      "Cost on training data: 0.0016846059158020937\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 333 training complete\n",
      "Cost on training data: 0.001679085977533867\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 334 training complete\n",
      "Cost on training data: 0.0016736164349617408\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 335 training complete\n",
      "Cost on training data: 0.0016681701061132346\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 336 training complete\n",
      "Cost on training data: 0.001662779129525204\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 337 training complete\n",
      "Cost on training data: 0.0016574036087997172\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 338 training complete\n",
      "Cost on training data: 0.0016520670307393283\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 339 training complete\n",
      "Cost on training data: 0.0016467381911902564\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 340 training complete\n",
      "Cost on training data: 0.0016414669914338448\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 341 training complete\n",
      "Cost on training data: 0.0016362183096070692\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 342 training complete\n",
      "Cost on training data: 0.0016310082814696993\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 343 training complete\n",
      "Cost on training data: 0.0016258280792235873\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 344 training complete\n",
      "Cost on training data: 0.0016206937728631568\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 345 training complete\n",
      "Cost on training data: 0.0016155791030036172\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 346 training complete\n",
      "Cost on training data: 0.00161050156123125\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 347 training complete\n",
      "Cost on training data: 0.0016054419537336027\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 348 training complete\n",
      "Cost on training data: 0.0016004130389257727\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 349 training complete\n",
      "Cost on training data: 0.0015954425136398644\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 350 training complete\n",
      "Cost on training data: 0.001590468820180691\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 351 training complete\n",
      "Cost on training data: 0.0015855415570309075\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 352 training complete\n",
      "Cost on training data: 0.0015806394992458196\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 353 training complete\n",
      "Cost on training data: 0.0015757572668423853\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 354 training complete\n",
      "Cost on training data: 0.0015709231928247686\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 355 training complete\n",
      "Cost on training data: 0.0015661106467558584\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 356 training complete\n",
      "Cost on training data: 0.0015613285568979673\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 357 training complete\n",
      "Cost on training data: 0.0015565717306181687\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 358 training complete\n",
      "Cost on training data: 0.001551833023892563\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 359 training complete\n",
      "Cost on training data: 0.0015471294562566307\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 360 training complete\n",
      "Cost on training data: 0.001542482543315464\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 361 training complete\n",
      "Cost on training data: 0.0015378108113408631\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 362 training complete\n",
      "Cost on training data: 0.0015331903841686635\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 363 training complete\n",
      "Cost on training data: 0.00152859539998335\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 364 training complete\n",
      "Cost on training data: 0.0015240352731166731\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 365 training complete\n",
      "Cost on training data: 0.0015194839104878508\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 366 training complete\n",
      "Cost on training data: 0.0015149898466536508\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 367 training complete\n",
      "Cost on training data: 0.0015104970804991375\n",
      "Accuracy on evaluation data: 822 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368 training complete\n",
      "Cost on training data: 0.0015060487230755943\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 369 training complete\n",
      "Cost on training data: 0.0015016142248072547\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 370 training complete\n",
      "Cost on training data: 0.0014971929437874525\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 371 training complete\n",
      "Cost on training data: 0.0014928024819473014\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 372 training complete\n",
      "Cost on training data: 0.0014884519822878641\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 373 training complete\n",
      "Cost on training data: 0.0014841199857266619\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 374 training complete\n",
      "Cost on training data: 0.0014797942902950877\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 375 training complete\n",
      "Cost on training data: 0.001475500330871552\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 376 training complete\n",
      "Cost on training data: 0.001471237886573365\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 377 training complete\n",
      "Cost on training data: 0.0014669973946550554\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 378 training complete\n",
      "Cost on training data: 0.0014627773754999265\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 379 training complete\n",
      "Cost on training data: 0.001458580400348885\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 380 training complete\n",
      "Cost on training data: 0.0014544040648391378\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 381 training complete\n",
      "Cost on training data: 0.0014502524274267435\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 382 training complete\n",
      "Cost on training data: 0.0014461250297869146\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 383 training complete\n",
      "Cost on training data: 0.001442033618273986\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 384 training complete\n",
      "Cost on training data: 0.0014379533324626102\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 385 training complete\n",
      "Cost on training data: 0.001433909737772526\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 386 training complete\n",
      "Cost on training data: 0.001429873011259284\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 387 training complete\n",
      "Cost on training data: 0.00142586958093672\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 388 training complete\n",
      "Cost on training data: 0.0014218645269413097\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 389 training complete\n",
      "Cost on training data: 0.0014178959683548474\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 390 training complete\n",
      "Cost on training data: 0.0014139315420069244\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 391 training complete\n",
      "Cost on training data: 0.0014100192016329011\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 392 training complete\n",
      "Cost on training data: 0.0014061007918060198\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 393 training complete\n",
      "Cost on training data: 0.0014022186064229084\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 394 training complete\n",
      "Cost on training data: 0.001398358892043106\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 395 training complete\n",
      "Cost on training data: 0.001394521364146941\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 396 training complete\n",
      "Cost on training data: 0.0013906980069793742\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 397 training complete\n",
      "Cost on training data: 0.0013868901615895724\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 398 training complete\n",
      "Cost on training data: 0.0013831052406849387\n",
      "Accuracy on evaluation data: 822 / 1000\n",
      "Epoch 399 training complete\n",
      "Cost on training data: 0.0013793387680806042\n",
      "Accuracy on evaluation data: 822 / 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [617,\n",
       "  694,\n",
       "  767,\n",
       "  755,\n",
       "  757,\n",
       "  778,\n",
       "  789,\n",
       "  800,\n",
       "  781,\n",
       "  796,\n",
       "  799,\n",
       "  795,\n",
       "  799,\n",
       "  798,\n",
       "  798,\n",
       "  804,\n",
       "  801,\n",
       "  806,\n",
       "  802,\n",
       "  808,\n",
       "  808,\n",
       "  807,\n",
       "  811,\n",
       "  804,\n",
       "  805,\n",
       "  805,\n",
       "  810,\n",
       "  805,\n",
       "  808,\n",
       "  809,\n",
       "  808,\n",
       "  812,\n",
       "  810,\n",
       "  810,\n",
       "  811,\n",
       "  810,\n",
       "  809,\n",
       "  811,\n",
       "  814,\n",
       "  810,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  811,\n",
       "  811,\n",
       "  811,\n",
       "  811,\n",
       "  811,\n",
       "  809,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  813,\n",
       "  813,\n",
       "  813,\n",
       "  811,\n",
       "  816,\n",
       "  814,\n",
       "  814,\n",
       "  813,\n",
       "  815,\n",
       "  812,\n",
       "  813,\n",
       "  815,\n",
       "  813,\n",
       "  815,\n",
       "  815,\n",
       "  812,\n",
       "  815,\n",
       "  813,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  813,\n",
       "  813,\n",
       "  812,\n",
       "  814,\n",
       "  813,\n",
       "  816,\n",
       "  812,\n",
       "  815,\n",
       "  813,\n",
       "  812,\n",
       "  812,\n",
       "  813,\n",
       "  813,\n",
       "  814,\n",
       "  813,\n",
       "  813,\n",
       "  812,\n",
       "  812,\n",
       "  814,\n",
       "  813,\n",
       "  813,\n",
       "  813,\n",
       "  813,\n",
       "  812,\n",
       "  812,\n",
       "  813,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  812,\n",
       "  812,\n",
       "  813,\n",
       "  815,\n",
       "  814,\n",
       "  814,\n",
       "  815,\n",
       "  813,\n",
       "  815,\n",
       "  813,\n",
       "  813,\n",
       "  814,\n",
       "  814,\n",
       "  813,\n",
       "  815,\n",
       "  814,\n",
       "  813,\n",
       "  815,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  814,\n",
       "  814,\n",
       "  813,\n",
       "  814,\n",
       "  813,\n",
       "  813,\n",
       "  814,\n",
       "  814,\n",
       "  813,\n",
       "  813,\n",
       "  814,\n",
       "  814,\n",
       "  814,\n",
       "  814,\n",
       "  815,\n",
       "  814,\n",
       "  815,\n",
       "  814,\n",
       "  814,\n",
       "  816,\n",
       "  817,\n",
       "  814,\n",
       "  814,\n",
       "  817,\n",
       "  816,\n",
       "  815,\n",
       "  815,\n",
       "  816,\n",
       "  815,\n",
       "  817,\n",
       "  815,\n",
       "  814,\n",
       "  816,\n",
       "  815,\n",
       "  816,\n",
       "  816,\n",
       "  816,\n",
       "  815,\n",
       "  816,\n",
       "  816,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  816,\n",
       "  818,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  818,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  818,\n",
       "  818,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  818,\n",
       "  817,\n",
       "  818,\n",
       "  818,\n",
       "  818,\n",
       "  818,\n",
       "  818,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  817,\n",
       "  818,\n",
       "  818,\n",
       "  817,\n",
       "  818,\n",
       "  818,\n",
       "  818,\n",
       "  817,\n",
       "  818,\n",
       "  818,\n",
       "  818,\n",
       "  818,\n",
       "  818,\n",
       "  817,\n",
       "  819,\n",
       "  818,\n",
       "  817,\n",
       "  819,\n",
       "  819,\n",
       "  819,\n",
       "  819,\n",
       "  818,\n",
       "  819,\n",
       "  818,\n",
       "  818,\n",
       "  818,\n",
       "  819,\n",
       "  819,\n",
       "  819,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  819,\n",
       "  819,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  820,\n",
       "  821,\n",
       "  820,\n",
       "  821,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  821,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  821,\n",
       "  822,\n",
       "  821,\n",
       "  821,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822,\n",
       "  822],\n",
       " [1.4860090005504503,\n",
       "  0.8280943195441068,\n",
       "  0.5366466416989764,\n",
       "  0.403756159940483,\n",
       "  0.3085568322759454,\n",
       "  0.24047572413134766,\n",
       "  0.1734732106297779,\n",
       "  0.15306847764062245,\n",
       "  0.13873407517143088,\n",
       "  0.09949106892934292,\n",
       "  0.0861098563746581,\n",
       "  0.08588264128582065,\n",
       "  0.08057020314104271,\n",
       "  0.06412690357074638,\n",
       "  0.057007320417624796,\n",
       "  0.05636996937438226,\n",
       "  0.04899514365465413,\n",
       "  0.045780613383313656,\n",
       "  0.044457387388562794,\n",
       "  0.041909051822022945,\n",
       "  0.038023942839024855,\n",
       "  0.03542708141594091,\n",
       "  0.03438673641117075,\n",
       "  0.03207797345911388,\n",
       "  0.0306379915765604,\n",
       "  0.029842836471050224,\n",
       "  0.027747991401994767,\n",
       "  0.026805623381960374,\n",
       "  0.02583137206903195,\n",
       "  0.024733692891839,\n",
       "  0.023809926066772978,\n",
       "  0.02290680118412933,\n",
       "  0.02216706481925767,\n",
       "  0.021330946494124023,\n",
       "  0.02058596223527525,\n",
       "  0.019936403066641177,\n",
       "  0.01938996346324361,\n",
       "  0.01872798221027592,\n",
       "  0.018242870854387385,\n",
       "  0.01772315087742149,\n",
       "  0.017291433653981874,\n",
       "  0.016770699400081312,\n",
       "  0.016319402998070203,\n",
       "  0.015916193218857986,\n",
       "  0.015488844470941449,\n",
       "  0.0151227226752806,\n",
       "  0.01478497914908075,\n",
       "  0.014424744271281579,\n",
       "  0.014099577552351625,\n",
       "  0.013778930671771905,\n",
       "  0.013474430337901062,\n",
       "  0.013170758762086366,\n",
       "  0.012943750226847845,\n",
       "  0.012608666228305894,\n",
       "  0.012416375624288377,\n",
       "  0.01214516153239222,\n",
       "  0.011915399712760955,\n",
       "  0.011631786233707789,\n",
       "  0.011410939167039504,\n",
       "  0.01121277573067292,\n",
       "  0.010985912492821754,\n",
       "  0.01079724510469282,\n",
       "  0.01060963850261874,\n",
       "  0.01043339017069026,\n",
       "  0.010280439363227914,\n",
       "  0.010074934469319854,\n",
       "  0.00991509037850457,\n",
       "  0.009745938082216923,\n",
       "  0.00958383195798235,\n",
       "  0.009436334303025392,\n",
       "  0.0092998081841727,\n",
       "  0.00914619049997124,\n",
       "  0.009010107832255832,\n",
       "  0.008868725258129215,\n",
       "  0.008738667706135262,\n",
       "  0.00860857454495136,\n",
       "  0.00848546245568806,\n",
       "  0.008372491425575869,\n",
       "  0.008256239836670941,\n",
       "  0.008135202094298369,\n",
       "  0.008026750553486157,\n",
       "  0.007912340064243024,\n",
       "  0.007812776498783147,\n",
       "  0.007706704179971372,\n",
       "  0.007602183694404931,\n",
       "  0.007504581310953663,\n",
       "  0.00741141715784975,\n",
       "  0.007316186143001194,\n",
       "  0.00722918598136662,\n",
       "  0.007144319199325253,\n",
       "  0.00705308311134918,\n",
       "  0.006975796830794571,\n",
       "  0.006883671864217525,\n",
       "  0.006799839305481967,\n",
       "  0.0067207811392139055,\n",
       "  0.006642694582703226,\n",
       "  0.0065715902718388695,\n",
       "  0.00649456614779212,\n",
       "  0.0064264775364375,\n",
       "  0.006349202944328813,\n",
       "  0.006280766364063707,\n",
       "  0.006212864529105994,\n",
       "  0.006147076541728791,\n",
       "  0.006079020910126854,\n",
       "  0.0060171595592770615,\n",
       "  0.0059539734305073195,\n",
       "  0.005893434372808843,\n",
       "  0.005837723243024336,\n",
       "  0.005771762728438364,\n",
       "  0.005716635737530476,\n",
       "  0.005657680855850348,\n",
       "  0.005602420435005678,\n",
       "  0.00554704922817219,\n",
       "  0.005493515299150136,\n",
       "  0.005441014936240289,\n",
       "  0.005390834313913964,\n",
       "  0.005339636684147621,\n",
       "  0.005288886905794968,\n",
       "  0.0052381816913798455,\n",
       "  0.0051927983948177,\n",
       "  0.005142661671939819,\n",
       "  0.005096937383336491,\n",
       "  0.005051069997846578,\n",
       "  0.005005730272522156,\n",
       "  0.004962561297957301,\n",
       "  0.004918123334006114,\n",
       "  0.004875024066388754,\n",
       "  0.004834427579104659,\n",
       "  0.004793123268166903,\n",
       "  0.004751393844793896,\n",
       "  0.004711870233694022,\n",
       "  0.0046711536004299635,\n",
       "  0.004633151677519255,\n",
       "  0.004594160567127169,\n",
       "  0.004556854846722735,\n",
       "  0.004520236671184956,\n",
       "  0.004483010948461276,\n",
       "  0.00444764662052496,\n",
       "  0.004412510849434112,\n",
       "  0.004376878352703191,\n",
       "  0.004342726109054858,\n",
       "  0.004309328539642442,\n",
       "  0.004275652995946331,\n",
       "  0.004243108988262437,\n",
       "  0.004212251081219184,\n",
       "  0.004179122412973893,\n",
       "  0.004147499637035717,\n",
       "  0.0041166954921723784,\n",
       "  0.004086216916705137,\n",
       "  0.004055702011084691,\n",
       "  0.004026487261793825,\n",
       "  0.003996695039947755,\n",
       "  0.003968026085300022,\n",
       "  0.003940351293773149,\n",
       "  0.003911257112385546,\n",
       "  0.003884147860876704,\n",
       "  0.003856710417840155,\n",
       "  0.0038299541324404093,\n",
       "  0.003802872767643012,\n",
       "  0.0037769695654307333,\n",
       "  0.003750648205435093,\n",
       "  0.003725303595316258,\n",
       "  0.0037003529847531623,\n",
       "  0.00367507909554692,\n",
       "  0.003650555132699767,\n",
       "  0.003626050575324353,\n",
       "  0.003602208273178827,\n",
       "  0.003578367390085992,\n",
       "  0.0035548296005908594,\n",
       "  0.003531936022282689,\n",
       "  0.0035092471792538254,\n",
       "  0.0034864162565246427,\n",
       "  0.003464149331840719,\n",
       "  0.003442184206225732,\n",
       "  0.003421009256987645,\n",
       "  0.003399776074507443,\n",
       "  0.0033781302264436278,\n",
       "  0.0033571757509373314,\n",
       "  0.00333609039161036,\n",
       "  0.0033156348994267883,\n",
       "  0.003295412958850589,\n",
       "  0.0032753055743751655,\n",
       "  0.0032554910776436572,\n",
       "  0.003236012548416205,\n",
       "  0.0032167963061466276,\n",
       "  0.0031983227856402416,\n",
       "  0.003178670809822685,\n",
       "  0.003160103155455913,\n",
       "  0.003141722198896312,\n",
       "  0.0031235852540828497,\n",
       "  0.003105324993008273,\n",
       "  0.003087356385405269,\n",
       "  0.003069740444953951,\n",
       "  0.0030521662988711065,\n",
       "  0.003035103339522621,\n",
       "  0.0030178643170641448,\n",
       "  0.0030008868226569993,\n",
       "  0.0029842368975238204,\n",
       "  0.0029677781369283063,\n",
       "  0.002951143357958223,\n",
       "  0.002935185612755114,\n",
       "  0.0029189238772388206,\n",
       "  0.00290315926216028,\n",
       "  0.0028874331976949715,\n",
       "  0.002871964128799633,\n",
       "  0.00285640560860618,\n",
       "  0.002841337781323861,\n",
       "  0.0028261066434823448,\n",
       "  0.0028113507073558234,\n",
       "  0.0027963923862072532,\n",
       "  0.0027819825223036956,\n",
       "  0.0027672196474394332,\n",
       "  0.0027528228214703035,\n",
       "  0.0027387296231636063,\n",
       "  0.002724710148602946,\n",
       "  0.0027107752053224764,\n",
       "  0.0026970430818257945,\n",
       "  0.0026832529538517867,\n",
       "  0.0026697258273466785,\n",
       "  0.0026563599604643258,\n",
       "  0.0026431054998058345,\n",
       "  0.002630115969441326,\n",
       "  0.0026169942435038795,\n",
       "  0.002604044933142382,\n",
       "  0.0025913000085283568,\n",
       "  0.0025786345594925658,\n",
       "  0.0025661289548268985,\n",
       "  0.0025536455671429903,\n",
       "  0.002541405357160823,\n",
       "  0.0025292313886642833,\n",
       "  0.0025172730038195415,\n",
       "  0.0025052131902663817,\n",
       "  0.002493346792981853,\n",
       "  0.002481706797298765,\n",
       "  0.002470100398863122,\n",
       "  0.0024585142801331664,\n",
       "  0.0024469939006431587,\n",
       "  0.0024356106513411554,\n",
       "  0.0024244640318756244,\n",
       "  0.0024132624056740727,\n",
       "  0.00240228807152328,\n",
       "  0.0023912944298381616,\n",
       "  0.002380414931706105,\n",
       "  0.002369734397461747,\n",
       "  0.002359130612975542,\n",
       "  0.002348482192209523,\n",
       "  0.0023380306276204626,\n",
       "  0.0023277815677281532,\n",
       "  0.0023174375120689947,\n",
       "  0.0023071222152245955,\n",
       "  0.002297172777776851,\n",
       "  0.0022871295998763202,\n",
       "  0.002277158202351896,\n",
       "  0.0022672509298639772,\n",
       "  0.0022575724220680457,\n",
       "  0.0022476990381330507,\n",
       "  0.0022381840133002835,\n",
       "  0.002228529845097418,\n",
       "  0.002219041950034404,\n",
       "  0.0022097313646171715,\n",
       "  0.0022003742466529335,\n",
       "  0.0021911896760226364,\n",
       "  0.0021819926376587103,\n",
       "  0.0021729132864321187,\n",
       "  0.0021639367813171832,\n",
       "  0.0021549587411979257,\n",
       "  0.002146078394093681,\n",
       "  0.0021372535347313126,\n",
       "  0.0021285537880688715,\n",
       "  0.0021199290631801334,\n",
       "  0.0021112606884334546,\n",
       "  0.002102759789059166,\n",
       "  0.0020943134090421495,\n",
       "  0.002085916914943167,\n",
       "  0.0020775766311441806,\n",
       "  0.0020692937125251086,\n",
       "  0.002061140454812675,\n",
       "  0.002052970875001824,\n",
       "  0.002044855853372872,\n",
       "  0.002036862517121067,\n",
       "  0.0020288958382185624,\n",
       "  0.002021005176527842,\n",
       "  0.0020131527934101395,\n",
       "  0.002005377283007738,\n",
       "  0.0019976816396060464,\n",
       "  0.0019900419144368263,\n",
       "  0.001982394557380568,\n",
       "  0.001974869269795398,\n",
       "  0.001967386646226768,\n",
       "  0.001959967544210675,\n",
       "  0.0019525751332373608,\n",
       "  0.0019452528944115745,\n",
       "  0.0019379433276990542,\n",
       "  0.001930708549982476,\n",
       "  0.0019235697192032137,\n",
       "  0.0019163996527454892,\n",
       "  0.00190932912335846,\n",
       "  0.0019023088684398197,\n",
       "  0.001895342286152582,\n",
       "  0.0018884094379651888,\n",
       "  0.001881537622707725,\n",
       "  0.0018747695136692548,\n",
       "  0.0018679846481921593,\n",
       "  0.001861208845394822,\n",
       "  0.0018545179941365304,\n",
       "  0.0018478940953408414,\n",
       "  0.0018413328970543373,\n",
       "  0.0018347782788468501,\n",
       "  0.0018282541840208915,\n",
       "  0.0018217974158265511,\n",
       "  0.001815365655454888,\n",
       "  0.0018089998071789627,\n",
       "  0.001802670886038719,\n",
       "  0.0017963834551463463,\n",
       "  0.0017901521905088673,\n",
       "  0.0017839543867796874,\n",
       "  0.001777811896929545,\n",
       "  0.0017716844477538188,\n",
       "  0.0017656004521386381,\n",
       "  0.0017595690507968987,\n",
       "  0.0017535856735146549,\n",
       "  0.0017476170994782392,\n",
       "  0.0017417535309568082,\n",
       "  0.001735827307003468,\n",
       "  0.0017299928602195808,\n",
       "  0.00172420173079514,\n",
       "  0.0017184240401416603,\n",
       "  0.0017127075306587899,\n",
       "  0.0017069948464568032,\n",
       "  0.0017013444954066814,\n",
       "  0.0016957691392599059,\n",
       "  0.001690154214809408,\n",
       "  0.0016846059158020937,\n",
       "  0.001679085977533867,\n",
       "  0.0016736164349617408,\n",
       "  0.0016681701061132346,\n",
       "  0.001662779129525204,\n",
       "  0.0016574036087997172,\n",
       "  0.0016520670307393283,\n",
       "  0.0016467381911902564,\n",
       "  0.0016414669914338448,\n",
       "  0.0016362183096070692,\n",
       "  0.0016310082814696993,\n",
       "  0.0016258280792235873,\n",
       "  0.0016206937728631568,\n",
       "  0.0016155791030036172,\n",
       "  0.00161050156123125,\n",
       "  0.0016054419537336027,\n",
       "  0.0016004130389257727,\n",
       "  0.0015954425136398644,\n",
       "  0.001590468820180691,\n",
       "  0.0015855415570309075,\n",
       "  0.0015806394992458196,\n",
       "  0.0015757572668423853,\n",
       "  0.0015709231928247686,\n",
       "  0.0015661106467558584,\n",
       "  0.0015613285568979673,\n",
       "  0.0015565717306181687,\n",
       "  0.001551833023892563,\n",
       "  0.0015471294562566307,\n",
       "  0.001542482543315464,\n",
       "  0.0015378108113408631,\n",
       "  0.0015331903841686635,\n",
       "  0.00152859539998335,\n",
       "  0.0015240352731166731,\n",
       "  0.0015194839104878508,\n",
       "  0.0015149898466536508,\n",
       "  0.0015104970804991375,\n",
       "  0.0015060487230755943,\n",
       "  0.0015016142248072547,\n",
       "  0.0014971929437874525,\n",
       "  0.0014928024819473014,\n",
       "  0.0014884519822878641,\n",
       "  0.0014841199857266619,\n",
       "  0.0014797942902950877,\n",
       "  0.001475500330871552,\n",
       "  0.001471237886573365,\n",
       "  0.0014669973946550554,\n",
       "  0.0014627773754999265,\n",
       "  0.001458580400348885,\n",
       "  0.0014544040648391378,\n",
       "  0.0014502524274267435,\n",
       "  0.0014461250297869146,\n",
       "  0.001442033618273986,\n",
       "  0.0014379533324626102,\n",
       "  0.001433909737772526,\n",
       "  0.001429873011259284,\n",
       "  0.00142586958093672,\n",
       "  0.0014218645269413097,\n",
       "  0.0014178959683548474,\n",
       "  0.0014139315420069244,\n",
       "  0.0014100192016329011,\n",
       "  0.0014061007918060198,\n",
       "  0.0014022186064229084,\n",
       "  0.001398358892043106,\n",
       "  0.001394521364146941,\n",
       "  0.0013906980069793742,\n",
       "  0.0013868901615895724,\n",
       "  0.0013831052406849387,\n",
       "  0.0013793387680806042],\n",
       " [])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "net = Network([784, 100, 10], cost=CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(list(training_data)[:1000], 400, 10, 0.5,\n",
    "        evaluation_data=list(test_data)[:1000],\n",
    "        monitor_evaluation_accuracy=True,\n",
    "        monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
